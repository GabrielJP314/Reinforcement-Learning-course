{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "Reinforcement Learning can be thought of as a third paradigm of statistical learning, separated from Supervised (e.g., Regression models) and unsupervised learning(e.g. clustering).\n",
    "\n",
    "One of the main characteristics of Reinforcement Learning is that it consists of a goal-directed agent, which interacts with an environment. The agent can observe the state of the environment, and perform actions that change the state of the environment. The agent also receives rewards from the environment, which are used to evaluate the agent's actions. The goal of the agent is to maximize the total reward it receives from the environment. The environment is uncertain. leading the agent to try to balance exploitation (leveraging previous successful experiences) *vs* exploration(trying out new strategies), in its decisions.\n",
    "\n",
    "Whenever the agent take an action, the environment is modified as a result. Therefore, the correct choice of action must take into account indirect delayed consequences of all previous actions. Thus the environment can be thought of as a Markov Decision Process (MDP).\n",
    "\n",
    "## Getting Started: the K-armed Bandit Problem\n",
    "The K-armed bandit problem is a simple example of a Reinforcement Learning problem. The agent is faced repeatedly with a choice. It can choose between K different actions, and receives a reward based on the action it chooses, the reward comes from a fixed probability distribution over the actions. The goal of the agent is to maximize the total reward it receives over time. The action chosen at time $t$ is denoted as $A_t$. The reward received is denoted as $R_t$. The expected value of an action $a$ is denoted as $q_*(a)$:\n",
    "\n",
    "$$q_*(a) = \\mathrm{E}[R_t \\mid A_t = a].$$\n",
    "\n",
    "The estimated value of an action $a$ at time $t$ is denoted as $Q_t(a)$. The value of an action is the expected reward given that action is selected. The value of an action is not known to the agent, and must be estimated based on the rewards received. The agent must balance exploration (trying out different actions to estimate their value) and exploitation (choosing the action with the highest estimated value). If the agent always chooses the action with the highest estimated value, we say that the agent is greedy. If the agent chooses a non-greedy action, we say that the agent is exploratory.\n",
    "\n",
    "## Action-value Methods\n",
    "Action-value methods estimate the value of each action, and select the action with the highest estimated value. The simplest action-value method is the sample-average method. The estimated value of an action $a$ at time $t$ is the average of the rewards received when $a$ was selected up to time $t$:\n",
    "\n",
    "$$Q_t(a)=\\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbb{1}_{A_t=a}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_t=a}}$$\n",
    "\n",
    "The greedy action is the action with the highest estimated value:\n",
    "$$A_t=argmax_a(Q_t(a))$$\n",
    "\n",
    "A simple alternative to the greedy action is the $\\epsilon$-greedy action. With probability $\\epsilon$, the agent chooses a random action. With probability $1-\\epsilon$, the agent chooses the greedy action. The $\\epsilon$-greedy action is a simple way to balance exploration and exploitation.\n",
    "\n",
    "In the code below we run a simulation of a 10-armed bandit that learns over 1000 decisions. This experiment is repeated 2000 times for multiple values of $\\epsilon$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../images/figure_2_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/Code/Sutton_Barto/chapter02/ten_armed_testbed.py:262\u001B[0m\n\u001B[1;32m    258\u001B[0m     plt\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 262\u001B[0m     \u001B[43mfigure_2_1\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m     figure_2_2()\n\u001B[1;32m    264\u001B[0m     figure_2_3()\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/Code/Sutton_Barto/chapter02/ten_armed_testbed.py:145\u001B[0m, in \u001B[0;36mfigure_2_1\u001B[0;34m()\u001B[0m\n\u001B[1;32m    143\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAction\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    144\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReward distribution\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 145\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msavefig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../images/figure_2_1.png\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m plt\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:1023\u001B[0m, in \u001B[0;36msavefig\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Figure\u001B[38;5;241m.\u001B[39msavefig)\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msavefig\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1022\u001B[0m     fig \u001B[38;5;241m=\u001B[39m gcf()\n\u001B[0;32m-> 1023\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msavefig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1024\u001B[0m     fig\u001B[38;5;241m.\u001B[39mcanvas\u001B[38;5;241m.\u001B[39mdraw_idle()  \u001B[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001B[39;00m\n\u001B[1;32m   1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/figure.py:3343\u001B[0m, in \u001B[0;36mFigure.savefig\u001B[0;34m(self, fname, transparent, **kwargs)\u001B[0m\n\u001B[1;32m   3339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes:\n\u001B[1;32m   3340\u001B[0m         stack\u001B[38;5;241m.\u001B[39menter_context(\n\u001B[1;32m   3341\u001B[0m             ax\u001B[38;5;241m.\u001B[39mpatch\u001B[38;5;241m.\u001B[39m_cm_set(facecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m, edgecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m-> 3343\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_figure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/backend_bases.py:2366\u001B[0m, in \u001B[0;36mFigureCanvasBase.print_figure\u001B[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001B[0m\n\u001B[1;32m   2362\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2363\u001B[0m     \u001B[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001B[39;00m\n\u001B[1;32m   2364\u001B[0m     \u001B[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001B[39;00m\n\u001B[1;32m   2365\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m cbook\u001B[38;5;241m.\u001B[39m_setattr_cm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure, dpi\u001B[38;5;241m=\u001B[39mdpi):\n\u001B[0;32m-> 2366\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mprint_method\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2367\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2368\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfacecolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfacecolor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2369\u001B[0m \u001B[43m            \u001B[49m\u001B[43medgecolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medgecolor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2370\u001B[0m \u001B[43m            \u001B[49m\u001B[43morientation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morientation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2371\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbbox_inches_restore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_bbox_inches_restore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2372\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2373\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2374\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m bbox_inches \u001B[38;5;129;01mand\u001B[39;00m restore_bbox:\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/backend_bases.py:2232\u001B[0m, in \u001B[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   2228\u001B[0m     optional_kws \u001B[38;5;241m=\u001B[39m {  \u001B[38;5;66;03m# Passed by print_figure for other renderers.\u001B[39;00m\n\u001B[1;32m   2229\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdpi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfacecolor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124medgecolor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morientation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2230\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox_inches_restore\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m   2231\u001B[0m     skip \u001B[38;5;241m=\u001B[39m optional_kws \u001B[38;5;241m-\u001B[39m {\u001B[38;5;241m*\u001B[39minspect\u001B[38;5;241m.\u001B[39msignature(meth)\u001B[38;5;241m.\u001B[39mparameters}\n\u001B[0;32m-> 2232\u001B[0m     print_method \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mwraps(meth)(\u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: \u001B[43mmeth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2233\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mskip\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   2234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Let third-parties do as they see fit.\u001B[39;00m\n\u001B[1;32m   2235\u001B[0m     print_method \u001B[38;5;241m=\u001B[39m meth\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:509\u001B[0m, in \u001B[0;36mFigureCanvasAgg.print_png\u001B[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprint_png\u001B[39m(\u001B[38;5;28mself\u001B[39m, filename_or_obj, \u001B[38;5;241m*\u001B[39m, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, pil_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    463\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;124;03m    Write the figure to a PNG file.\u001B[39;00m\n\u001B[1;32m    465\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;124;03m        *metadata*, including the default 'Software' key.\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 509\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_print_pil\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename_or_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpng\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpil_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:458\u001B[0m, in \u001B[0;36mFigureCanvasAgg._print_pil\u001B[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001B[0m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001B[39;00m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    457\u001B[0m FigureCanvasAgg\u001B[38;5;241m.\u001B[39mdraw(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 458\u001B[0m \u001B[43mmpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimsave\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename_or_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer_rgba\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfmt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morigin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mupper\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdpi\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfigure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdpi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpil_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpil_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/matplotlib/image.py:1689\u001B[0m, in \u001B[0;36mimsave\u001B[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001B[0m\n\u001B[1;32m   1687\u001B[0m pil_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1688\u001B[0m pil_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdpi\u001B[39m\u001B[38;5;124m\"\u001B[39m, (dpi, dpi))\n\u001B[0;32m-> 1689\u001B[0m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpil_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/PIL/Image.py:2429\u001B[0m, in \u001B[0;36mImage.save\u001B[0;34m(self, fp, format, **params)\u001B[0m\n\u001B[1;32m   2427\u001B[0m         fp \u001B[38;5;241m=\u001B[39m builtins\u001B[38;5;241m.\u001B[39mopen(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr+b\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2428\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2429\u001B[0m         fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mw+b\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2431\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2432\u001B[0m     save_handler(\u001B[38;5;28mself\u001B[39m, fp, filename)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../images/figure_2_1.png'"
     ]
    }
   ],
   "source": [
    "%run ../Code/Sutton_Barto/chapter02/ten_armed_testbed.py"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-14T15:59:22.575524858Z",
     "start_time": "2023-06-14T15:59:21.075411359Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To efficiently implement action-value methods, we can use the incremental update rule, to estimate the Average reward at step $n$\n",
    "$$Q_{n+1}=Q_n + \\frac{1}{n}\\left[R_n - Q_n \\right],$$\n",
    "\n",
    "where $R_n$ is the reward at step $n$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonstationary Reward Problems\n",
    "In the previous example, the reward distribution was stationary. The expected reward of each action did not change over time. In the nonstationary case, the expected reward of each action changes over time. Let $\\alpha$ be a constant step size.\n",
    "\n",
    "\\begin{align}\n",
    "Q_{n+1} &= Q_n + \\alpha \\left[R_n - Q_n \\right] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha) Q_n \\\\\n",
    "&= \\alpha R_n + (1-\\alpha) \\left[ \\alpha R_{n-1} + (1-\\alpha) Q_{n-1} \\right] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha) \\alpha R_{n-1} + (1-\\alpha)^2 Q_{n-1} \\\\\n",
    "&= \\dots\\\\\n",
    "&= (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i\n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

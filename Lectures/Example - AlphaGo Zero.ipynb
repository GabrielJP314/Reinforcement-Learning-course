{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing AlphaGo Zero\n",
    "AlphaGo Zero is a computer program developed by DeepMind to play the board game Go. It is the successor to AlphaGo, the first computer program to defeat a professional human Go player, the first to defeat a Go world champion, and arguably the strongest Go player in history. The program became a [stronger player](https://en.wikipedia.org/wiki/Go_ranks_and_ratings) than any human, and arguably the strongest Go player in history, after just a few days of self-training.  The cost for the hardware and computing power used to train AlphaGo Zero was estimated at over 25 million US dollars.\n",
    "\n",
    "\n",
    "This introduction is based on the Deepmind's Nature paper *[Mastering the game of Go without human knowledge](https://rdcu.be/dmAC7)*.\n",
    "\n",
    "## AlphaGo Zero's differences from AlphaGo and AlphaGo Lee\n",
    "AlphaGo Zero differs from AlphaGo and AlphaGo Lee in several aspects. First, AlphaGo Zero is trained solely by self-play reinforcement learning, starting from random play, without any supervision or use of human data. Second, it learns to master not just one game, but three: Go, shogi, and chess. Finally, in contrast to all previous game-playing programs, AlphaGo Zero's neural network is trained and evaluated from scratch, starting only from the game rules, with no human data used in any fashion. \n",
    "\n",
    "AlphaGo Zero simplified the board representation by only using the black and white stones in the board as features. It also uses a single neural network, rather than separate policy and value networks. The neural network takes as input a representation of the board position as a feature map, and outputs a vector of move probabilities and a scalar value evaluation of the current position. The neural network is trained to predict the probability of winning from each position, and the final move is selected by a Monte Carlo Tree Search (MCTS) algorithm. The MCTS rollouts are no longer executed. The Neural network itself evaluates move look-ahead.\n",
    "\n",
    "## AlphaGo Zero's training\n",
    "The training is exclusively done by self-play reinforcement learning, starting from random play, without any supervision or use of human data. From each position $s$ An MCTS search is conducted based on the move probabilities generated by the NN. This search allows the finding of much stronger moves than would normally be found by the neural network alone. The reinforcement learning used is a form of policy iteration procedure, where MCTS is used for both policy evaluation and policy improvement. \n",
    "\n",
    "### Policy iteration\n",
    "Policy iteration is a general reinforcement learning method that iteratively improves a policy $\\pi$ until it converges to an optimal policy $\\pi^*$. The policy iteration procedure consists of two steps: policy evaluation and policy improvement. In the policy evaluation step, the value function $v_\\pi$ of the current policy $\\pi$ is computed. In the policy improvement step, the policy is improved by acting greedily with respect to the value function $v_\\pi$. The policy iteration procedure is guaranteed to converge to the optimal policy $\\pi^*$. \n",
    "\n",
    "## AlphaGo Zero's performance\n",
    "Over the course of the Training, 29 million games of self-play where generated and from these games, the program was able to discover most of the Go knowledge that has been accumulated by human players over thousands of years. AlphaGo Zero reached an Elo rating of 5185, higher than the 4858 rating of AlphaGo Master. In fact AlphaGo Zero was able to beat AlphaGo Master 89 to 11 in a 100-game match to 2h limit in duration.\n",
    "\n",
    "# AlphaZero's differences from AlphaGo Zero\n",
    "AlphaZero differs from AlphaGo Zero in several aspects. First, AlphaZero learns to master not just one game, but three: Go, shogi, and chess. Second, AlphaZero learns to master these three games without any human knowledge or data. After only 34 hours of training, AlphaZero was able to defeat AlphaGo Zero 60 games to 40 in a 100-game match to 2h limit in duration. AlphaZero also defeated the world champion chess program Stockfish 8 in a 100-game match to 2h limit in duration. AlphaZero won 28 games, lost none, and drew 72 games.\n",
    "\n",
    "# Leela Zero\n",
    "[Leela Zero](https://zero.sjeng.org/) is an open-source, community-based project attempting to replicate the approach of AlphaZero. It is based on the Go engine Leela, and is a fork of the Leela Zero repository. The project started in April 2018, and is led by Belgian programmer Gian-Carlo Pascutto. The project is supported by donations from the community, and its network is open source, available under the GPLv3 license. The project is currently in beta phase, and has been used to run several distributed training runs, some of which have been used to generate a new network.\n",
    "\n",
    "# MuZero\n",
    "At the time of the publication of the [MuZero paper](https://www.nature.com/articles/s41586-020-03051-4.pdf), MuZero was the most efficient algorithm for learning the rules of a game and predicting the winner. MuZero is a general-purpose reinforcement learning algorithm that is able to master a range of challenging tasks, including the game of Go, chess, shogi and Atari games, **without knowing the rules of the game**. MuZero *learns a model* that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games, without incorporating any human knowledge, MuZero achieved a new state-of-the-art performance in 43 of them, and surpassed the previous state-of-the-art in 14 of them. When evaluated on Go, chess and shogi, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.\n",
    "\n",
    "## MuZero's Algorithm\n",
    "MuZero's algorithm makes predictions at each step $t$, for $k=0\\ldots K$ steps ahead, by a model $\\mu_\\theta$, conditioned on past observations $o_{1:t}$ and for $k>0$ on future actions $a_{t+1:t+k}$, where $\\theta$ are the model parameters. The model is trained to minimize the error in predicting the true values of the quantities $r^k_t$, $p^k_t$ and $v^k_t$ at each step $t$.\n",
    "\n",
    "\\begin{align}\n",
    "r^k_t &\\approx u_{t+k} \\\\\n",
    "p^k_t(a_t) &\\approx \\pi(a_{t+k+1}\\mid o_1,\\ldots, o_t, a_{t+1},\\ldots, a_{t+k}) \\\\\n",
    "v^k_t &\\approx \\mathbb{E} \\left[ u_{t+k+1} + \\gamma u_{t+k+2} + \\ldots \\mid o_1,\\ldots, o_t, a_{t+1},\\ldots, a_{t+k} \\right]\n",
    "\\end{align}\n",
    "\n",
    "where $r_t$ is the reward, $p_t(a_t)$ is the probability of selecting action $a_t$ at step $t$, and $v_t$ is the value function at step $t$. The model is trained to minimize the loss function $L(\\theta)$:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) = \\sum_{k=0}^K l^p(\\pi_{t+k},p^k_t) + \\sum_{k=0}^K l^v(z_{t+k},v^k_t) + \\sum_{k=0}^K l^r(u_{t+k},r^k_t) + c||\\theta||^2\n",
    "\\end{align}\n",
    "\n",
    "where $l^p$, $l^v$ and $l^r$ are the loss functions for the policy, value and reward, respectively, and $c||\\theta||^2$ is a regularization term. The policy loss $l^p$ is the cross-entropy between the predicted policy $\\pi_{t+k}$ and the target policy $p^k_t$. The value loss $l^v$ is the mean squared error between the predicted value $v^k_t$ and the target value $z_{t+k}$, which is the discounted sum of rewards $u_{t+k}$, with discount factor $\\gamma$. The reward loss $l^r$ is the mean squared error between the predicted reward $u_{t+k}$ and the target reward $r^k_t$. The model is trained by minimizing the loss function $L(\\theta)$ using stochastic gradient descent.\n",
    "\n",
    "## Other MuZero's resources\n",
    "- [MuZero's paper](https://www.nature.com/articles/s41586-020-03051-4.pdf)\n",
    "- [MuZero's blog post](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules)\n",
    "- [MuZero's pseudo-code](https://doi.org/10.1038/s41586-020-03051-4)\n",
    "- [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

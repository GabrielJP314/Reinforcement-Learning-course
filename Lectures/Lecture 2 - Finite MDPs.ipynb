{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "%run?\n",
    "# Finite Markov Decision Processes\n",
    "MDPs are a classical formalization of sequential decision-making. They can handle not only immediate rewards but also rewards that are received at a later stage of the process. Differently from the bandit problems, in MDPs we estimate the value of $q_*(s,a)$, which is the expected return starting from state $s$, taking action $a$ that is, now the action's value also depends on the state $s$ of the system. The optimal value function $v_*(s)$ of a state, is the maximum value function over all policies.\n",
    "\n",
    "MDPs define the learning problem as the continued interation of an agent and its environment.\n",
    "\n",
    "<img src=\"Reinforcement_learning_diagram.png\" alt= \"Agent-Environment interaction\" width=\"400\" height=\"400\">\n",
    "\n",
    "As part of the interactions the agent receives a reward and observes the next state of the environment. So for every time step $t$, the agent receives the state $S_t \\in \\cal{S}$, takes an action $A_t \\in \\cal{A}(s)$ and receives a numerical reward $R_{t+1} \\in \\cal{R} \\subset{\\mathbb{R}}$ in the next step as a consequence. So the process generates the following seguence:\n",
    "\n",
    "$$S_0 , A_0 , R_1 , S_1 , A_1 , R_2 , S_2 , A_2 , R_3 ,\\ldots$$\n",
    "\n",
    "In a finite MDP, the sets of states, actions and rewards ($\\cal{S}$, $\\cal{A}$, $\\cal{R}$) are all finite. The dynamics of the MDP are given by the state-transition probabilities $$p(s',r|s,a) = Pr\\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\\},$$ which are the probabilities of the next state $s'$ and next reward $r$ given the current state $s$ and action $a$. The state-transition probabilities completely characterize any finite MDP.\n",
    "\n",
    "The  expected rewards for state-action pairs are defined as:\n",
    "\\begin{equation}\n",
    "r(s,a) = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \\sum_{r \\in \\cal{R}} r \\sum_{s' \\in \\cal{S}} p(s',r|s,a)\n",
    "\\end{equation}\n",
    "\n",
    "We define the environment as everything that is beyond the agent's control.\n",
    "\n",
    "## Goals and Rewards\n",
    "The  goal of the agent is to maximize the total amount of reward it receives. The agent does not care about the immediate reward only, but also about the long-term rewards. Using a reward signal to represent the goal is a fundamental feature of reinforcement learning.\n",
    "\n",
    "### Episodes and Continuous Tasks\n",
    "Some learning tasks can be structure in the form of episodes, where the total reward of each episode, denoted $G_T$, is defined as the sum of the rewards up to the last time step $T$:\n",
    "\n",
    "$$G_t = R_{t+1} + R_{t+2} + \\ldots + R_T$$\n",
    "\n",
    "this type of tasks are called *episodic tasks*. In other tasks, the agent-environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. These are called *continuing tasks*. In these tasks where $T=\\infty$ the total reward may also be infinite. To make it easier to talk about these tasks, we define the *discounted return* at time step $t$ as:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "where $\\gamma$ is a parameter, $0 \\leq \\gamma \\leq 1$, called the *discount rate*. The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. The present value of future rewards is called the *return*. The value of receiving reward $R$ after $k+1$ time steps is $\\gamma^k R$. If $\\gamma < 1$, the infinite sum has a finite value whenever the rewards are bounded. If $\\gamma = 0$, the agent is myopic, it only cares about the immediate reward. If $\\gamma = 1$, the agent cares about the entire future trajectory of rewards.\n",
    "\n",
    "## Policies and Value Functions\n",
    "A policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a|s)$ is the probability that $A_t=a$ if $S_t=s$. If the state and action spaces are finite, then a policy can be described by a matrix $\\pi$ where each entry $\\pi(s,a)$ is the probability of taking action $a$ in state $s$. The set of all policies is denoted $\\Pi$.\n",
    "\n",
    "The value function of a state $s$ under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define the value function for state-action pairs as well. The value function of state-action pair $(s,a)$ under a policy $\\pi$, denoted $q_{\\pi}(s,a)$, is the expected return starting from $s$, taking action $a$ and thereafter following policy $\\pi$. The value functions are defined as:\n",
    "\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s\\right], \\, \\forall s \\in \\cal{S} \\\\\n",
    "q_{\\pi}(s,a) &= \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\\\\\n",
    "&= \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t=s, A_t=a\\right], \\, \\forall s \\in \\cal{S}, a \\in \\cal{A}(s)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbb{E}_{\\pi}$ denotes the expected value of a random variable given that the agent follows policy $\\pi$. The value functions are specific to the policy $\\pi$, so we can also write them as $v_{\\pi}(s)$ and $q_{\\pi}(s,a)$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../Code/Sutton_Barto/chapter03')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T16:43:02.470518131Z",
     "start_time": "2023-06-21T16:43:02.460325617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'grid_world.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/IPython/core/magics/execution.py:701\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    700\u001B[0m     fpath \u001B[38;5;241m=\u001B[39m arg_lst[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 701\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[43mfile_finder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/IPython/utils/path.py:90\u001B[0m, in \u001B[0;36mget_py_filename\u001B[0;34m(name)\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m py_name\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile `\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m` not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m name)\n",
      "\u001B[0;31mOSError\u001B[0m: File `'grid_world.py'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgrid_world.py\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2417\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2415\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2416\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2417\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2419\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2420\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2421\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m~/MEGAsync/Cursos/Reinforcement Learning/Reinforcement-Learning-course/.venv/lib/python3.11/site-packages/IPython/core/magics/execution.py:712\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.*\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m\"\u001B[39m,fpath):\n\u001B[1;32m    711\u001B[0m         warn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFor Windows, use double quotes to wrap a filename: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124mun \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmypath\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmyfile.py\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 712\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    713\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    714\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fpath \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmeta_path:\n",
      "\u001B[0;31mException\u001B[0m: File `'grid_world.py'` not found."
     ]
    }
   ],
   "source": [
    "%run grid_world.py"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T16:48:30.348784909Z",
     "start_time": "2023-06-21T16:48:29.997875942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Finite Markov Decision Processes\n",
    "MDPs are a classical formalization of sequential decision-making. They can handle not only immediate rewards but also rewards that are received at a later stage of the process. Differently from the bandit problems, in MDPs we estimate the value of $q_*(s,a)$, which is the expected return starting from state $s$, taking action $a$ that is, now the action's value also depends on the state $s$ of the system. The optimal value function $v_*(s)$ of a state, is the maximum value function over all policies.\n",
    "\n",
    "MDPs define the learning problem as the continued interation of an agent and its environment.\n",
    "\n",
    "<img src=\"Reinforcement_learning_diagram.png\" alt= \"Agent-Environment interaction\" width=\"400\" height=\"400\">\n",
    "\n",
    "As part of the interactions the agent receives a reward and observes the next state of the environment. So for every time step $t$, the agent receives the state $S_t \\in \\cal{S}$, takes an action $A_t \\in \\cal{A}(s)$ and receives a numerical reward $R_{t+1} \\in \\cal{R} \\subset{\\mathbb{R}}$ in the next step as a consequence. So the process generates the following seguence:\n",
    "\n",
    "$$S_0 , A_0 , R_1 , S_1 , A_1 , R_2 , S_2 , A_2 , R_3 ,\\ldots$$\n",
    "\n",
    "In a finite MDP, the sets of states, actions and rewards ($\\cal{S}$, $\\cal{A}$, $\\cal{R}$) are all finite. The dynamics of the MDP are given by the state-transition probabilities $$p(s',r|s,a) = Pr\\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\\},$$ which are the probabilities of the next state $s'$ and next reward $r$ given the current state $s$ and action $a$. The state-transition probabilities completely characterize any finite MDP.\n",
    "\n",
    "The  expected rewards for state-action pairs are defined as:\n",
    "\\begin{equation}\n",
    "r(s,a) = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \\sum_{r \\in \\cal{R}} r \\sum_{s' \\in \\cal{S}} p(s',r|s,a)\n",
    "\\end{equation}\n",
    "\n",
    "We define the environment as everything that is beyond the agent's control.\n",
    "\n",
    "## Goals and Rewards\n",
    "The  goal of the agent is to maximize the total amount of reward it receives. The agent does not care about the immediate reward only, but also about the long-term rewards. Using a reward signal to represent the goal is a fundamental feature of reinforcement learning.\n",
    "\n",
    "### Episodes and Continuous Tasks\n",
    "Some learning tasks can be structure in the form of episodes, where the total reward of each episode, denoted $G_T$, is defined as the sum of the rewards up to the last time step $T$:\n",
    "\n",
    "$$G_t = R_{t+1} + R_{t+2} + \\ldots + R_T$$\n",
    "\n",
    "this type of tasks are called *episodic tasks*. In other tasks, the agent-environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. These are called *continuing tasks*. In these tasks where $T=\\infty$ the total reward may also be infinite. To make it easier to talk about these tasks, we define the *discounted return* at time step $t$ as:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "where $\\gamma$ is a parameter, $0 \\leq \\gamma \\leq 1$, called the *discount rate*. The discount rate determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. The present value of future rewards is called the *return*. The value of receiving reward $R$ after $k+1$ time steps is $\\gamma^k R$. If $\\gamma < 1$, the infinite sum has a finite value whenever the rewards are bounded. If $\\gamma = 0$, the agent is myopic, it only cares about the immediate reward. If $\\gamma = 1$, the agent cares about the entire future trajectory of rewards.\n",
    "\n",
    "## Policies and Value Functions\n",
    "A policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a|s)$ is the probability that $A_t=a$ if $S_t=s$. If the state and action spaces are finite, then a policy can be described by a matrix $\\pi$ where each entry $\\pi(s,a)$ is the probability of taking action $a$ in state $s$. The set of all policies is denoted $\\Pi$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

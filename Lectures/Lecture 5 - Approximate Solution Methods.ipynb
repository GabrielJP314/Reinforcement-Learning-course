{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Solution Methods\n",
    "In order to avoid the combinatory explosion of tabular mehods, when the number of states and/or actions is large, we must resort to approximate methods.\n",
    "Approximate methods are based on the idea of representing the value function as a parametrized function, and then updating the parameters to minimize the error between the true value function and the approximated one.\n",
    "\n",
    "## Policy Gradient Methods\n",
    "Policy gradient methods are a class of reinforcement learning algorithms that rely on estimating the gradient of the policy, and then updating the policy parameters in the direction of the gradient. Let's denote the policy parameters as $\\theta$, and the policy as $\\pi(a\\mid s,\\theta)= Pr\\{A_t=a\\mid S_t=s, \\theta_t=\\theta\\}$. We will attempt to learn $\\theta$ such that some performance measure $J(\\theta)$ is maximized. \n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "In policy gradient methods, the performance measure $J(\\theta)$ is usually the value function $v_\\pi(s)$, the state-action value function $q_\\pi(s,a)$, or some other function that is related to the value function. \n",
    "\n",
    "### Policy Gradient Theorem\n",
    "The policy gradient theorem states that the gradient of the performance measure $J(\\theta)$ is given by\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J(\\theta) = \\sum_s \\mu_\\pi(s) \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a\\mid s,\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_\\pi(s)$ is the stationary distribution of the Markov chain induced by the policy $\\pi$.\n",
    "\n",
    "### REINFORCE\n",
    "The REINFORCE algorithm is a Monte Carlo policy gradient algorithm that uses the policy gradient theorem to update the policy parameters. The algorithm is as follows: \n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha G_t \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a\\mid s,\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $G_t$ is the return at time $t$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

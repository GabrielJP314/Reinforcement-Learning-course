{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Q-learning, SARSA and Temporal Difference methods\n",
    "Q-learning defines an algorithm to approximate $q_*$, the *optimal action-value function*. We can write it independently of the policy being followed, and it is guaranteed to converge to $q_*$ as long as all **state-action pairs** are visited infinitely often and the policy converges in the limit to the greedy policy.\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right].\n",
    "\\end{equation}\n",
    "\n",
    "In procedural form, the algorithm is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\text{Initialize } Q(s, a), \\text{ for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s), \\text{ arbitrarily except that } Q(\\texttt{terminal}, \\cdot) = 0 \\\\\n",
    "& \\text{Repeat (for each episode):} \\\\\n",
    "& \\quad \\text{Initialize } S \\\\\n",
    "& \\quad \\text{Repeat (for each step of episode):} \\\\\n",
    "& \\quad \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "& \\quad \\quad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "& \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_a Q(S', a) - Q(S, A) \\right] \\\\\n",
    "& \\quad \\quad S \\leftarrow S' \\\\\n",
    "& \\quad \\text{until } S \\text{ is terminal}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Q-learning is called an **off-policy** because it directly approximates $q_*$, the optimal action-value function, independent of the policy being followed. \n",
    "\n",
    "## SARSA\n",
    "SARSA is an **on-policy** TD control algorithm. It is very similar to Q-learning, but instead of using the greedy policy to select the next action, it uses the same policy that is being learned about. The update rule is:\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right].\n",
    "\\end{equation}\n",
    "\n",
    "In procedural form, the algorithm is:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\text{Initialize } Q(s, a), \\text{ for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s), \\text{ arbitrarily except that } Q(\\texttt{terminal}, \\cdot) = 0 \\\\\n",
    "& \\text{Repeat (for each episode):} \\\\\n",
    "& \\quad \\text{Initialize } S \\\\\n",
    "& \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "& \\quad \\text{Repeat (for each step of episode):} \\\\\n",
    "& \\quad \\quad \\text{Take action } A, \\text{ observe } R, S', A' \\\\\n",
    "& \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma Q(S', A') - Q(S, A) \\right] \\\\\n",
    "& \\quad \\quad S \\leftarrow S'; A \\leftarrow A' \\\\\n",
    "& \\quad \\text{until } S \\text{ is terminal}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    " The main difference of SARSA from Q-learning is that the choice action $A'$ is based on policy $\\pi$, instead of the greedy action $\\max_a Q(S', a)$.\n",
    "**Example 6.6: Cliff Walking**\n",
    "Run this example described on page 132 of the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected SARSA\n",
    "Expected SARSA is an alternative to Q-learning that is more robust to noise and variance in the estimates of the action values. It is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\mathbb{E}_\\pi \\left[ Q(S_{t+1}, A_{t+1}) \\mid S_{t+1} \\right] - Q(S_t, A_t) \\right]\\\\\n",
    "&\\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \\right].\n",
    "\\end{align}\n",
    "\n",
    "The above update rule, uses the expected value instead of the maximum value over the next state,action pairs. This is useful when the action values are noisy, and the maximum value may not be representative of the true value of the action. The expected value is more robust to noise, and is less likely to overestimate the action values. Otherwise this rule is very similar to Q-learning, updating at every time step. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods\n",
    "Temporal-difference is one on the most important innovations in reinforcement learning. It is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). \n",
    "\n",
    "## TD Prediction\n",
    "The prediction problem can be described as follows: given some experience and a policy $\\pi$, estimate the value function $v_\\pi$ for the nonterminal states $S_t$ occurring in that experience. Contrary to Monte Carlo methods, TD methods update estimates at each time step instead of waiting until the end of the episode. The simplest TD method is TD(0), which is defined by the update rule:\n",
    "\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the step-size parameter. The quantity $R_{t+1} + \\gamma V(S_{t+1})$ is called the TD target. The error in the TD update is $R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$, which is the TD error. The TD target is an estimate of the return $G_t$ and, like Monte Carlo methods, TD(0) converges to $v_\\pi$ as the number of updates approaches infinity.\n",
    "\n",
    "## Advantages of TD Prediction Methods\n",
    "TD methods have several advantages over Monte Carlo methods:\n",
    "- TD methods can learn before knowing the final outcome. Monte Carlo methods must wait until the end of the episode to learn.\n",
    "- TD methods can learn without the final outcome. Monte Carlo methods only learn from complete episodes.\n",
    "- TD can learn without the model of the environment's dynamics. Monte Carlo methods require a model of the environment.\n",
    "\n",
    "**Example 6.2: Random Walk**\n",
    "Run the code for this example described in page 125 of the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
